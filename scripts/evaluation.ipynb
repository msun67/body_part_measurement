{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from train import BodyPartsMeasurementTrainer\n",
    "from body_parts_measurement_data_generator import BodyPartsMeasurementDataGenerator\n",
    "import tensorflow as tf\n",
    "from evaluator import Evaluator\n",
    "from model.model import get_model\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # model\n",
    "    'input_shape': [256,256,3],     # 훈련 이미지 크기\n",
    "    'batch_size': 4,                # 배치 사이즈\n",
    "    'path_pretrained': None,        # pretrained 모델 경로\n",
    "    'type_backbone': \"blazepose\",   # backbone type (blazepose, mobilenet_v3)\n",
    "    \n",
    "    # loss\n",
    "    'type_loss_fn': 'wing',         # 손실 함수 설정 (wing, mae)\n",
    "    \n",
    "    # data\n",
    "    'seg_shape': [64,64],           # segmentation 크기 *미사용\n",
    "    'path_classes': \"../seg_classes.txt\",   # segmentation class 정보 *미사용\n",
    "    'shuffle': True,                # 데이터 섞기\n",
    "    'is_normalized': False,         # normalize 데이터\n",
    "    'is_with_seg': False,           # segmentation 사용 여부 *미사용\n",
    "    'path_dataset': \"D:\\\\data\\\\body_parts_measurement\", # 데이터 경로\n",
    "    ## attention type              \n",
    "    'type_attention': \"regression\", # attention 종류 (regression, categorical, none)\n",
    "    'num_category_bmi': 10,         # categorical 시의 bmi category 갯수 변수\n",
    "    'num_category_height': 10,      # categorical 시의 height category 갯수 변수\n",
    "\n",
    "    # train\n",
    "    'epochs': 30,                   # 훈련 epoch 수\n",
    "    'eval_term': 1                  # 평가 빈도\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('blazepose_attention_31features_731train_mf/blazepose_attention_0_2.994556342230903.h5')\n",
    "model.load_weights('blazepose_attention_31features_1002train_mf/blazepose_attention_0_3.2034787193590604.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list_user_dirs ['F004', 'F022', 'F027', 'F037', 'F057', 'F060', 'F061', 'F063', 'F064', 'F069', 'F088', 'F089', 'F098', 'F109', 'F114', 'F121', 'F129', 'F134', 'F170', 'F188', 'F192', 'F206', 'F214', 'F217', 'F224', 'F226', 'F239', 'F258', 'F259', 'F308', 'F312', 'F314', 'F324', 'F325', 'F327', 'F329', 'F332', 'F333', 'F344', 'F352', 'F356', 'F364', 'F379', 'F380', 'F383', 'F413', 'F419', 'F423', 'F426', 'F442', 'F492', 'F493', 'M009', 'M022', 'M024', 'M027', 'M041', 'M056', 'M065', 'M072', 'M074', 'M081', 'M088', 'M090', 'M101', 'M113', 'M125', 'M126', 'M128', 'M139', 'M153', 'M167', 'M172', 'M173', 'M175', 'M195', 'M199', 'M221', 'M225', 'M240', 'M255', 'M260', 'M301', 'M306', 'M308', 'M310', 'M315', 'M336', 'M338', 'M345', 'M353', 'M362', 'M365', 'M372', 'M384', 'M401', 'M441', 'M450', 'M462', 'M464', 'M465', 'M466', 'M473', 'M485', 'M491']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30240"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_generator_test = BodyPartsMeasurementDataGenerator(config, data_type=\"test\")\n",
    "len(data_generator_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(data_generator_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 24681/30240 [2:13:57<27:52,  3.32it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error OpenCV(4.5.3) C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-req-build-c2l3r8zm\\opencv\\modules\\imgproc\\src\\resize.cpp:4051: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30240/30240 [2:44:44<00:00,  3.06it/s]\n"
     ]
    }
   ],
   "source": [
    "evaluator.run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.12476265917276143\n",
      "1 0.041094798135945965\n",
      "2 0.061170808992874554\n",
      "3 0.11555510882491876\n",
      "4 0.13487792100704843\n",
      "5 0.04244377068651675\n",
      "6 0.24284584491640093\n",
      "7 0.03797059532622887\n",
      "8 0.03504382386216877\n",
      "9 0.1750653760514688\n",
      "10 0.083661787484856\n",
      "11 0.07641139240750593\n",
      "12 0.06533858123513538\n",
      "13 0.19006077327604928\n",
      "14 0.0453961984237471\n",
      "15 0.04583019170669868\n",
      "16 0.07960025961348301\n",
      "17 0.035635785065126435\n",
      "18 0.034620783848326134\n",
      "19 0.04251038172498105\n",
      "20 0.03571259517770826\n",
      "21 0.04167059543366567\n",
      "22 0.03893934871177923\n",
      "23 0.08076866443680712\n",
      "24 0.04811084598017413\n",
      "25 0.033569452438636245\n",
      "26 0.05361421186706462\n",
      "27 0.05744858358475839\n",
      "28 0.030285196556262016\n",
      "29 0.03638407207185011\n",
      "30 0.04266762423082188\n",
      "mape 0.0712602591048958\n"
     ]
    }
   ],
   "source": [
    "np_diff_parts_measurements = np.squeeze(np.array(evaluator.list_diff_parts_measurements))\n",
    "np_diff_percentage_parts_measurements = np.squeeze(np.array(evaluator.list_diff_percentage_parts_measurements))\n",
    "list_part_mapes = []\n",
    "for idx in range(np_diff_parts_measurements.shape[1]) :\n",
    "    part_mape = sum(np_diff_percentage_parts_measurements[:,idx])/np_diff_parts_measurements.shape[0]\n",
    "    print(idx, part_mape)\n",
    "    #print(sum(np_diff_percentage_parts_measurements[:,idx])/np_diff_parts_measurements.shape[0])\n",
    "    list_part_mapes.append(part_mape)\n",
    "print(\"mape\",sum(list_part_mapes)/len(list_part_mapes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_data_generator_test = iter(data_generator_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter_data_generator_test)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data[0][0,:,:,::-1].astype(np.int))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict([data[0], data[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(1, 256, 256, 3)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (1, 128, 128, 24)    672         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_144 (Sequential)     (1, 128, 128, 24)    840         conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_18 (TFOpLa (1, 128, 128, 24)    0           conv2d_118[0][0]                 \n",
      "                                                                 sequential_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_4 (TFOpLambda)       (1, 128, 128, 24)    0           tf.__operators__.add_18[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_145 (Sequential)     (1, 128, 128, 24)    840         tf.nn.relu_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_19 (TFOpLa (1, 128, 128, 24)    0           tf.nn.relu_4[0][0]               \n",
      "                                                                 sequential_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_5 (TFOpLambda)       (1, 128, 128, 24)    0           tf.__operators__.add_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "blaze_block_18 (BlazeBlock)     (1, 64, 64, 48)      9936        tf.nn.relu_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "blaze_block_19 (BlazeBlock)     (1, 32, 32, 96)      46272       blaze_block_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "blaze_block_20 (BlazeBlock)     (1, 16, 16, 192)     214464      blaze_block_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "blaze_block_21 (BlazeBlock)     (1, 8, 8, 288)       574176      blaze_block_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sequential_172 (Sequential)     (1, 16, 16, 48)      16752       blaze_block_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sequential_173 (Sequential)     (1, 16, 16, 48)      11184       blaze_block_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_20 (TFOpLa (1, 16, 16, 48)      0           sequential_172[0][0]             \n",
      "                                                                 sequential_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_9 (UpSampling2D)  (1, 32, 32, 48)      0           tf.__operators__.add_20[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_174 (Sequential)     (1, 32, 32, 48)      5616        blaze_block_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_21 (TFOpLa (1, 32, 32, 48)      0           up_sampling2d_9[0][0]            \n",
      "                                                                 sequential_174[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_10 (UpSampling2D) (1, 64, 64, 48)      0           tf.__operators__.add_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_175 (Sequential)     (1, 64, 64, 48)      2832        blaze_block_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_22 (TFOpLa (1, 64, 64, 48)      0           up_sampling2d_10[0][0]           \n",
      "                                                                 sequential_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(1, 2)]             0                                            \n",
      "__________________________________________________________________________________________________\n",
      "blaze_block_22 (BlazeBlock)     (1, 32, 32, 96)      46272       tf.__operators__.add_22[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "regression_conv12b (Sequential) (1, 32, 32, 96)      10272       blaze_block_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (1, 1024)            3072        input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_24 (TFOpLa (1, 32, 32, 96)      0           blaze_block_22[0][0]             \n",
      "                                                                 regression_conv12b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.reshape_8 (TFOpLambda)       (1, 32, 32, 1)       0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_6 (TFOpLambda)        (1, 32, 32, 97)      0           tf.__operators__.add_24[0][0]    \n",
      "                                                                 tf.reshape_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "blaze_block_23 (BlazeBlock)     (1, 16, 16, 192)     214666      tf.concat_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "regression_conv13b (Sequential) (1, 16, 16, 192)     38976       blaze_block_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (1, 256)             262400      dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_25 (TFOpLa (1, 16, 16, 192)     0           blaze_block_23[0][0]             \n",
      "                                                                 regression_conv13b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.reshape_9 (TFOpLambda)       (1, 16, 16, 1)       0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_7 (TFOpLambda)        (1, 16, 16, 193)     0           tf.__operators__.add_25[0][0]    \n",
      "                                                                 tf.reshape_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "blaze_block_24 (BlazeBlock)     (1, 8, 8, 289)       578196      tf.concat_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "regression_conv14b (Sequential) (1, 8, 8, 289)       86401       blaze_block_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (1, 64)              16448       dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_26 (TFOpLa (1, 8, 8, 289)       0           blaze_block_24[0][0]             \n",
      "                                                                 regression_conv14b[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.reshape_10 (TFOpLambda)      (1, 8, 8, 1)         0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_8 (TFOpLambda)        (1, 8, 8, 290)       0           tf.__operators__.add_26[0][0]    \n",
      "                                                                 tf.reshape_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "regression_conv15 (Sequential)  (1, 2, 2, 290)       1396640     tf.concat_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "joints (Sequential)             (1, 1, 1, 31)        35991       regression_conv15[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.reshape_11 (TFOpLambda)      (1, 31)              0           joints[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,572,918\n",
      "Trainable params: 3,572,918\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "list_user_dirs ['F004', 'F022', 'F027', 'F037', 'F057', 'F060', 'F061', 'F063', 'F064', 'F069', 'F088', 'F089', 'F098', 'F109', 'F114', 'F121', 'F129', 'F134', 'F170', 'F188', 'F192', 'F206', 'F214', 'F217', 'F224', 'F226', 'F239', 'F258', 'F259', 'F308', 'F312', 'F314', 'F324', 'F325', 'F327', 'F329', 'F332', 'F333', 'F344', 'F352', 'F356', 'F364', 'F379', 'F380', 'F383', 'F413', 'F419', 'F423', 'F426', 'F442', 'F492', 'F493', 'M009', 'M022', 'M024', 'M027', 'M041', 'M056', 'M065', 'M072', 'M074', 'M081', 'M088', 'M090', 'M101', 'M113', 'M125', 'M126', 'M128', 'M139', 'M153', 'M167', 'M172', 'M173', 'M175', 'M195', 'M199', 'M221', 'M225', 'M240', 'M255', 'M260', 'M301', 'M306', 'M308', 'M310', 'M315', 'M336', 'M338', 'M345', 'M353', 'M362', 'M365', 'M372', 'M384', 'M401', 'M441', 'M450', 'M462', 'M464', 'M465', 'M466', 'M473', 'M485', 'M491']\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "        # model\n",
    "        'input_shape': [256,256,3],\n",
    "        'batch_size': 1,\n",
    "        'path_pretrained': None, \n",
    "        'type_backbone': \"blazepose\", \n",
    "        \n",
    "        # loss\n",
    "        'type_loss_fn': 'wing',\n",
    "        \n",
    "        # data\n",
    "        'seg_shape': [64,64],\n",
    "        'path_classes': \"../seg_classes.txt\",\n",
    "        'shuffle': True, \n",
    "        'is_normalized': False, \n",
    "        'is_with_seg': False, \n",
    "        'path_dataset': \"D:\\\\data\\\\body_parts_measurement\",\n",
    "        ## attention type\n",
    "        'type_attention': \"regression\", #regression, categorical, none\n",
    "        'num_category_bmi': 10,\n",
    "        'num_category_height': 10,\n",
    "        'has_filename': True,\n",
    "\n",
    "        # train\n",
    "        'epochs': 30,\n",
    "        'eval_term': 1\n",
    "    }\n",
    "model = get_model(config)\n",
    "model.summary()\n",
    "model.load_weights('blazepose_attention_31features_1002train_mf/blazepose_attention_0_3.2034787193590604.h5')\n",
    "data_generator_test = BodyPartsMeasurementDataGenerator(config, data_type=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 20601/120960 [33:07<2:26:53, 11.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 20600 wrong\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120960/120960 [3:19:16<00:00, 10.12it/s] \n"
     ]
    }
   ],
   "source": [
    "f_w = open(\"eval_results_test.txt\",\"w\")\n",
    "f_w.write(\"Evaluation Started At \"+datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') + \"\\n\")\n",
    "\n",
    "list_mape = []\n",
    "list_diff_percentage_parts_measurements = []\n",
    "\n",
    "\n",
    "\n",
    "for idx in tqdm(range(len(data_generator_test))) :\n",
    "    try :\n",
    "        batch_data = data_generator_test.get_data(idx)\n",
    "    except :\n",
    "        print(\"idx\",idx,\"wrong\")\n",
    "    batch_images, batch_body_parts_measurement, batch_categorical_bmi_and_height, filename = batch_data\n",
    "    batch_images, batch_body_parts_measurement, batch_categorical_bmi_and_height = np.expand_dims(batch_images, axis=0), np.expand_dims(batch_body_parts_measurement, axis=0), np.expand_dims(batch_categorical_bmi_and_height, axis=0)\n",
    "\n",
    "    pd_parts_measurements = model.predict([batch_images, batch_categorical_bmi_and_height])\n",
    "    batch_diff_parts_measurements = abs(batch_body_parts_measurement - pd_parts_measurements)\n",
    "    batch_diff_percentage_parts_measurements = abs(batch_diff_parts_measurements / batch_body_parts_measurement)\n",
    "    \n",
    "\n",
    "    mape = sum(batch_diff_percentage_parts_measurements[0]) / len(batch_diff_percentage_parts_measurements[0])\n",
    "\n",
    "    str_filename = str(idx) + \" filename: \" + filename + \"\\n\"\n",
    "    str_gt = \"gt: \" + \", \".join(str(x) for x in batch_body_parts_measurement[0]) + \"\\n\"\n",
    "    str_pd = \"pd: \" + \", \".join(str(x) for x in pd_parts_measurements[0]) + \"\\n\"\n",
    "    str_pe = \"ape: \" + \", \".join(str(x) for x in batch_diff_percentage_parts_measurements[0]) + \"\\n\"\n",
    "    str_mape = \"MAPE : \" + str(mape) + \"\\n\\n\"\n",
    "\n",
    "    f_w.write(str_filename)\n",
    "    f_w.write(str_gt)\n",
    "    f_w.write(str_pd)\n",
    "    f_w.write(str_pe)\n",
    "    f_w.write(str_mape)\n",
    "\n",
    "    list_mape.append(mape)\n",
    "    list_diff_percentage_parts_measurements.append(batch_diff_percentage_parts_measurements)\n",
    "\n",
    "np_diff_percentage_parts_measurements = np.squeeze(np.array(list_diff_percentage_parts_measurements))\n",
    "for idx in range(np_diff_percentage_parts_measurements.shape[1]) :\n",
    "    part_mape = sum(np_diff_percentage_parts_measurements[:,idx])/np_diff_percentage_parts_measurements.shape[0]\n",
    "    f_w.write(\"Part MAPE \"+ str(idx) + \": \" + str(part_mape) + \"\\n\")\n",
    "\n",
    "f_w.write(\"MAPE for Test Dataset : \" + str(sum(list_mape)/len(list_mape)) + \"\\n\")\n",
    "f_w.write(\"Evaluation Ended At \"+datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') + \"\\n\")\n",
    "f_w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d10a1d14f906cf696e46041477fc2b17fecaf759e8e084262933cf48dc10c4e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('py375tf26g': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
